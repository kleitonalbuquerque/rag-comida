# üçù RAG Comida

Projeto **RAG (Retrieval-Augmented Generation)** focado em **conte√∫dos culin√°rios**, utilizando **FastAPI**, **PostgreSQL + pgvector** e **embeddings sem√¢nticos** para permitir buscas por significado (e n√£o apenas por palavras-chave).

O objetivo √© armazenar textos (receitas, dicas, curiosidades, descri√ß√µes de pratos etc.), gerar embeddings vetoriais e permitir consultas inteligentes que retornem os conte√∫dos mais relevantes com base na similaridade sem√¢ntica, **reduzindo alucina√ß√µes** e garantindo respostas baseadas em dados reais.

---

## üß† O que esse projeto faz

* Armazena conte√∫dos textuais relacionados a comida
* Gera embeddings sem√¢nticos (vetores de 384 dimens√µes)
* Salva os vetores no PostgreSQL usando **pgvector**
* Executa **busca sem√¢ntica** por similaridade vetorial
* Exp√µe endpoints REST usando **FastAPI**
* Serve como base para um pipeline **RAG completo**

---

## üõ†Ô∏è Tecnologias utilizadas

* **Python 3.11**
* **FastAPI**
* **Uvicorn**
* **PostgreSQL 15**
* **pgvector**
* **SQLAlchemy**
* **Sentence-Transformers** (embeddings)
* **Docker & Docker Compose**

---

## üìÇ Estrutura do projeto

```text
rag-comida/
‚îú‚îÄ‚îÄ app/                 # Backend FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # Entrypoint, /health e /search
‚îÇ   ‚îú‚îÄ‚îÄ db/              # Conex√£o, modelos e init do pgvector
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ ingest.py        # Script de ingest√£o usando all-MiniLM-L6-v2
‚îú‚îÄ‚îÄ frontend/            # Frontend Next.js (chat UI)
‚îÇ   ‚îú‚îÄ‚îÄ app/page.js      # Chat com fetch para /search
‚îÇ   ‚îî‚îÄ‚îÄ app/layout.js
‚îú‚îÄ‚îÄ docker-compose.yml   # Postgres + API
‚îú‚îÄ‚îÄ Dockerfile           # Build da API
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias da API
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Pr√©-requisitos

Antes de come√ßar, voc√™ precisa ter instalado:

* Python **3.11+**
* Docker
* Docker Compose
* Git

---

## üê≥ Subindo com Docker (API + Postgres)

Na raiz do projeto:

```bash
docker compose up -d
```

Isso sobe Postgres (pgvector habilitado) e a API. Para acompanhar:

```bash
docker compose ps
docker compose logs -f api
```

#### LLM (Ollama) acess√≠vel pelo container

- Inicie o Ollama ouvindo em todas as interfaces (no host ou no WSL):
	```bash
	OLLAMA_HOST=0.0.0.0:11434 ollama serve
	```
- A API usa endpoint compat√≠vel OpenAI em `http://host.docker.internal:11434/v1` (config no `.env`).
- Ap√≥s subir, fa√ßa um ping de aquecimento para evitar cold start:
	```bash
	docker compose exec api curl -s -X POST http://localhost:8000/chat \
		-H "Content-Type: application/json" \
		-d '{"message":"Diga apenas: ok","top_k":1}'
	```

> Se quiser verificar sa√∫de do LLM via API: `docker compose exec api curl -s http://localhost:8000/llm/health`

#### Acessar o banco (opcional)

```bash
docker exec -it rag-postgres psql -U postgres -d vector_db
```

---

## üîê Vari√°veis de ambiente

Crie um arquivo `.env` baseado no exemplo:

```bash
cp .env.example .env
```

Principais chaves usadas atualmente:

```env
DB_HOST=postgres
DB_PORT=5432
DB_NAME=vector_db
DB_USER=postgres
DB_PASSWORD=postgres

# LLM (Ollama via API compat√≠vel OpenAI)
LLM_BASE_URL=http://host.docker.internal:11434/v1
LLM_API_KEY=sk-local
LLM_MODEL=llama3.2:3b
LLM_TIMEOUT=60
```

> Dica: se o Ollama estiver no WSL, mantenha-o ouvindo em `0.0.0.0:11434` e use `host.docker.internal` no `.env` da API. Se estiver em outro host/IP, ajuste `LLM_BASE_URL` conforme.

---

## üêç Configurando o ambiente Python

### 1Ô∏è‚É£ Criar e ativar o ambiente virtual

**Windows (PowerShell):**

```powershell
python -m venv venv
. venv\Scripts\Activate.ps1
```

**Linux / Mac:**

```bash
python -m venv venv
source venv/bin/activate
```

---

### 2Ô∏è‚É£ Instalar depend√™ncias

```bash
python -m pip install -r requirements.txt
```

---

## üöÄ Rodando a aplica√ß√£o (API)

Com o ambiente virtual ativado (ou via container):

```bash
python -m uvicorn app.main:app --reload
```

Ou com Docker Compose:

```bash
docker compose up -d
```

A API fica em http://127.0.0.1:8000 (docs: http://127.0.0.1:8000/docs).

---

## ‚ù§Ô∏è Endpoint de sa√∫de

Para validar se est√° tudo funcionando corretamente:

```http
GET /health
```

Resposta esperada:

```json
{ "status": "ok" }
```

---

## üß≠ Frontend (Next.js + styled-components)

Interface de chat que consome `POST /chat` da API.

### Pr√©-requisitos
- Node 18+
- API rodando em http://localhost:8000 (ou defina `NEXT_PUBLIC_API_URL`)

### Setup
```bash
cd frontend
npm install
npm run dev
# abre em http://localhost:3000
```

Se a API estiver em outro host/porta, crie `frontend/.env.local`:
```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

### Como funciona
- Header com t√≠tulo e bot√£o ‚ÄúLimpar chat‚Äù.
- Input e bot√£o de envio travam enquanto o fetch est√° em andamento.
- Mensagens listadas em chat (usu√°rio/bot) com hor√°rio.
- Respostas do bot renderizam Markdown (negrito, listas, etc.).
- Faz `POST /chat` com `{ message, top_k }` e retorna resposta + fontes.

---

## üß† Fluxo RAG (resumo)

1. Textos culin√°rios s√£o ingeridos na base
2. Cada texto gera um embedding vetorial
3. Os vetores s√£o armazenados no PostgreSQL
4. O usu√°rio faz uma pergunta
5. A pergunta vira um embedding
6. O pgvector retorna os textos semanticamente mais pr√≥ximos
7. Esses textos s√£o usados por um LLM (via /chat) para gerar respostas confi√°veis

---

## üß© Pr√≥ximas etapas do projeto

* [ ] Servi√ßo de gera√ß√£o de embeddings
* [ ] Endpoint de ingest√£o de documentos
* [ ] Endpoint de busca sem√¢ntica
* [ ] Integra√ß√£o com LLM (RAG completo)
* [ ] Testes automatizados
* [ ] Frontend simples (chat ou busca)

---

## üìå Observa√ß√£o

Este projeto foi criado com foco **did√°tico e pr√°tico**, servindo como base para estudos e aplica√ß√µes reais de **RAG com banco vetorial** usando pgvector.

---

üöÄ **Pronto! O projeto agora pode ser facilmente entendido, instalado e evolu√≠do por qualquer pessoa do time.**
